{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Deep Neural Network\n",
    "\n",
    "A deep neural network (DNN) is a type of artificial neural network (ANN) with multiple layers between the input and output layers. Each layer in a DNN consists of nodes (also called neurons or units) that are connected to nodes in adjacent layers. The connections between nodes are represented by weights, and each connection has an associated weight that determines the strength of the connection. The basic building blocks of a DNN are neurons and the connections between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step explanation of how a simple feedforward deep neural network works:\n",
    "\n",
    "1. Input Layer: The input layer is where the network receives the initial data. Each node in the input layer represents a feature of the input data.\n",
    "\n",
    "2. Hidden Layers: Between the input and output layers, there are one or more hidden layers. Each node in a hidden layer receives input from the nodes in the previous layer, applies a weighted sum, and passes the result through an activation function. The activation function introduces non-linearity to the model, enabling it to learn complex patterns.\n",
    "\n",
    "3. Output Layer: The output layer produces the final prediction or classification. The number of nodes in the output layer depends on the problem type. For example, in binary classification, there might be one node with a sigmoid activation function, while in multi-class classification, there would be multiple nodes with softmax activation.\n",
    "\n",
    "4. Weights and Bias: Each connection between nodes has an associated weight, which determines the strength of that connection. Additionally, each node has a bias term, which is a constant that is added to the weighted sum before applying the activation function.\n",
    "\n",
    "5. Activation Function: The activation function introduces non-linearity to the model, allowing it to learn complex patterns. Common activation functions include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-dl-1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
